{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1imo1fxDSilQf63R_yaI0OnOt_4aQMRte",
      "authorship_tag": "ABX9TyM0esUdM0Rg5E6fU45Y6keQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arbin4/DM_DW_LAB/blob/main/Lab_2_DW_DM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
        "\n",
        "# New file paths\n",
        "file_paths = {\n",
        "    'space.txt': '/content/drive/MyDrive/DM_DW_Datas/space.txt',\n",
        "    'sports.txt': '/content/drive/MyDrive/DM_DW_Datas/sports.txt'\n",
        "}\n",
        "\n",
        "min_support = 0.15\n",
        "min_confidence = 0.7\n",
        "\n",
        "def get_support(itemset, transactions):\n",
        "    count = sum(1 for tx in transactions if itemset.issubset(set(tx)))\n",
        "    return count / len(transactions)\n",
        "\n",
        "def apriori(transactions, min_support):\n",
        "    total_tx = len(transactions)\n",
        "    item_counts = defaultdict(int)\n",
        "\n",
        "    for tx in transactions:\n",
        "        for item in tx:\n",
        "            item_counts[frozenset([item])] += 1\n",
        "\n",
        "    frequent_itemsets = {item: count for item, count in item_counts.items() if count / total_tx >= min_support}\n",
        "    all_frequent = frequent_itemsets.copy()\n",
        "    current_freq = list(frequent_itemsets.keys())\n",
        "    k = 2\n",
        "\n",
        "    while current_freq:\n",
        "        candidates = set()\n",
        "        for i in range(len(current_freq)):\n",
        "            for j in range(i + 1, len(current_freq)):\n",
        "                union = current_freq[i] | current_freq[j]\n",
        "                if len(union) == k:\n",
        "                    candidates.add(union)\n",
        "\n",
        "        candidate_counts = defaultdict(int)\n",
        "        for tx in transactions:\n",
        "            tx_set = set(tx)\n",
        "            for candidate in candidates:\n",
        "                if candidate.issubset(tx_set):\n",
        "                    candidate_counts[candidate] += 1\n",
        "\n",
        "        current_freq = [item for item in candidate_counts if candidate_counts[item] / total_tx >= min_support]\n",
        "        all_frequent.update({item: candidate_counts[item] for item in current_freq})\n",
        "        k += 1\n",
        "\n",
        "    return all_frequent\n",
        "\n",
        "def generate_rules(frequent_itemsets, transactions, min_confidence):\n",
        "    total_tx = len(transactions)\n",
        "    rules = []\n",
        "    for itemset in frequent_itemsets:\n",
        "        if len(itemset) < 2:\n",
        "            continue\n",
        "        support_itemset = frequent_itemsets[itemset] / total_tx\n",
        "        for i in range(1, len(itemset)):\n",
        "            for antecedent in combinations(itemset, i):\n",
        "                antecedent = frozenset(antecedent)\n",
        "                consequent = itemset - antecedent\n",
        "                support_ante = get_support(antecedent, transactions)\n",
        "                support_cons = get_support(consequent, transactions)\n",
        "                confidence = support_itemset / support_ante\n",
        "                lift = confidence / support_cons\n",
        "                if confidence >= min_confidence:\n",
        "                    rules.append({\n",
        "                        'antecedents': set(antecedent),\n",
        "                        'consequents': set(consequent),\n",
        "                        'support': round(support_itemset, 2),\n",
        "                        'confidence': round(confidence, 2),\n",
        "                        'lift': round(lift, 2)\n",
        "                    })\n",
        "    return rules\n",
        "\n",
        "for name, path in file_paths.items():\n",
        "    print(f\"\\n===== Processing {name} =====\")\n",
        "\n",
        "    # Regex-based line extraction\n",
        "    lines = re.findall(r'\\d+,[^\\n]+', open(path, encoding=\"utf-8\", errors=\"ignore\").read())\n",
        "    transactions = [[item.strip() for item in line.split(',')[1:] if item.strip()] for line in lines]\n",
        "\n",
        "    # Apriori timing and result\n",
        "    start_apriori = time.time()\n",
        "    frequent_itemsets_raw = apriori(transactions, min_support)\n",
        "    rules = generate_rules(frequent_itemsets_raw, transactions, min_confidence)\n",
        "    end_apriori = time.time()\n",
        "\n",
        "    # Output frequent itemsets\n",
        "    total_tx = len(transactions)\n",
        "    frequent_itemsets_df = pd.DataFrame([{\n",
        "        'itemsets': set(item),\n",
        "        'support': round(count / total_tx, 2)\n",
        "    } for item, count in frequent_itemsets_raw.items()])\n",
        "\n",
        "    rules_df = pd.DataFrame(rules)\n",
        "\n",
        "    print(\"\\nFrequent Itemsets:\\n\", frequent_itemsets_df)\n",
        "\n",
        "    if not rules_df.empty:\n",
        "        print(\"\\nAssociation Rules:\\n\", rules_df[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
        "    else:\n",
        "        print(\"\\nNo association rules found with confidence ≥\", min_confidence)\n",
        "\n",
        "    # FP-Growth timing and result\n",
        "    start_fp = time.time()\n",
        "    te = TransactionEncoder()\n",
        "    te_ary = te.fit(transactions).transform(transactions)\n",
        "    df_fp = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "    fp_itemsets = fpgrowth(df_fp, min_support=min_support, use_colnames=True)\n",
        "    fp_rules = association_rules(fp_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
        "    end_fp = time.time()\n",
        "\n",
        "    print(\"\\nFP-Growth Frequent Itemsets:\\n\", fp_itemsets)\n",
        "    if not fp_rules.empty:\n",
        "        print(\"\\nFP-Growth Association Rules:\\n\", fp_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
        "    else:\n",
        "        print(\"\\nNo association rules found using FP-Growth with confidence ≥\", min_confidence)\n",
        "\n",
        "    print(f\"\\nExecution Time (Apriori): {round(end_apriori - start_apriori, 4)} seconds\")\n",
        "    print(f\"Execution Time (FP-Growth): {round(end_fp - start_fp, 4)} seconds\")\n",
        "\n",
        "    print(\"\\n=== Comparison Summary ===\")\n",
        "    print(f\"Apriori generated {len(rules)} rules\")\n",
        "    print(f\"FP-Growth generated {len(fp_rules)} rules\")\n",
        "    if (end_apriori - start_apriori) > (end_fp - start_fp):\n",
        "        print(\"FP-Growth is faster than Apriori.\")\n",
        "    else:\n",
        "        print(\"Apriori is faster than FP-Growth.\")\n",
        "    print(\"Both algorithms generated similar types of association rules, but FP-Growth is generally more efficient for large datasets.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nc5gyVSCa4x",
        "outputId": "bfb10fc4-bd6d-4844-acf9-5d32f1f778ee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Processing space.txt =====\n",
            "\n",
            "Frequent Itemsets:\n",
            "                      itemsets  support\n",
            "0               {Robotic Arm}     0.34\n",
            "1              {Food Packets}     0.40\n",
            "2              {Sleeping Bag}     0.32\n",
            "3                 {Treadmill}     0.28\n",
            "4                {Space Suit}     0.32\n",
            "5                {3D Printer}     0.28\n",
            "6  {Carbon Dioxide Scrubbers}     0.24\n",
            "\n",
            "No association rules found with confidence ≥ 0.7\n",
            "\n",
            "FP-Growth Frequent Itemsets:\n",
            "    support                    itemsets\n",
            "0     0.40              (Food Packets)\n",
            "1     0.34               (Robotic Arm)\n",
            "2     0.32              (Sleeping Bag)\n",
            "3     0.28                 (Treadmill)\n",
            "4     0.32                (Space Suit)\n",
            "5     0.28                (3D Printer)\n",
            "6     0.24  (Carbon Dioxide Scrubbers)\n",
            "\n",
            "No association rules found using FP-Growth with confidence ≥ 0.7\n",
            "\n",
            "Execution Time (Apriori): 0.0012 seconds\n",
            "Execution Time (FP-Growth): 0.0667 seconds\n",
            "\n",
            "=== Comparison Summary ===\n",
            "Apriori generated 0 rules\n",
            "FP-Growth generated 0 rules\n",
            "Apriori is faster than FP-Growth.\n",
            "Both algorithms generated similar types of association rules, but FP-Growth is generally more efficient for large datasets.\n",
            "\n",
            "===== Processing sports.txt =====\n",
            "\n",
            "Frequent Itemsets:\n",
            "          itemsets  support\n",
            "0      {football}     0.44\n",
            "1  {cricket ball}     0.36\n",
            "2        {gloves}     0.36\n",
            "3   {cricket bat}     0.40\n",
            "4         {juice}     0.42\n",
            "5  {water bottle}     0.28\n",
            "6     {ice cream}     0.26\n",
            "\n",
            "No association rules found with confidence ≥ 0.7\n",
            "\n",
            "FP-Growth Frequent Itemsets:\n",
            "    support        itemsets\n",
            "0     0.44      (football)\n",
            "1     0.36        (gloves)\n",
            "2     0.36  (cricket ball)\n",
            "3     0.42         (juice)\n",
            "4     0.40   (cricket bat)\n",
            "5     0.28  (water bottle)\n",
            "6     0.26     (ice cream)\n",
            "\n",
            "No association rules found using FP-Growth with confidence ≥ 0.7\n",
            "\n",
            "Execution Time (Apriori): 0.0004 seconds\n",
            "Execution Time (FP-Growth): 0.016 seconds\n",
            "\n",
            "=== Comparison Summary ===\n",
            "Apriori generated 0 rules\n",
            "FP-Growth generated 0 rules\n",
            "Apriori is faster than FP-Growth.\n",
            "Both algorithms generated similar types of association rules, but FP-Growth is generally more efficient for large datasets.\n"
          ]
        }
      ]
    }
  ]
}